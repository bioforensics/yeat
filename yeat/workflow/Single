# -------------------------------------------------------------------------------------------------
# Copyright (c) 2023, DHS. This file is part of YEAT: http://github.com/bioforensics/yeat
#
# This software was prepared for the Department of Homeland Security (DHS) by the Battelle National
# Biodefense Institute, LLC (BNBI) as part of contract HSHQDC-15-C-00064 to manage and operate the
# National Biodefense Analysis and Countermeasures Center (NBACC), a Federally Funded Research and
# Development Center.
# -------------------------------------------------------------------------------------------------

from shutil import copyfile
from yeat.workflow.aux import *


rule fastp:
    output:
        filtered_reads="seq/fastp/{sample}/single/combined-reads.fq.gz",
    input:
        raw_reads="seq/input/{sample}/single/combined-reads.fq.gz"
    params:
        length_required=config["length_required"],
        html_report="seq/fastp/{sample}/single/fastp.html",
        json_report="seq/fastp/{sample}/single/fastp.json",
        txt_report="seq/fastp/{sample}/single/report.txt"
    shell:
        """
        fastp -i {input.raw_reads} -o {output.filtered_reads} \
            -l {params.length_required} \
            --html {params.html_report} --json {params.json_report} \
            2> {params.txt_report}
        """


rule mash:
    output:
        sketch="seq/mash/{sample}/single/combined-reads.fq.gz.msh",
        mash_report="seq/mash/{sample}/single/combined-reads.report.tsv"
    input:
        reads="seq/fastp/{sample}/single/combined-reads.fq.gz"
    shell:
        """
        mash sketch {input.reads} -o {output.sketch}
        mash info -t {output.sketch} > {output.mash_report}
        """


rule downsample:
    output:
        sub="seq/downsample/{sample}/single/combined-reads.fq.gz"
    input:
        reads="seq/fastp/{sample}/single/combined-reads.fq.gz",
        mash_report="seq/mash/{sample}/single/combined-reads.report.tsv"
    params:
        coverage_depth=lambda wildcards: config["samples"][wildcards.sample].coverage_depth,
        downsample=lambda wildcards: config["samples"][wildcards.sample].downsample,
        fastp_report="seq/fastp/{sample}/single/fastp.json",
        genome_size=lambda wildcards: config["samples"][wildcards.sample].genome_size,
        seed=config["seed"],
        outdir="seq/downsample/{sample}/single"
    run:
        if params.downsample == -1:
            copyfile(input.reads, output.sub)
            return
        genome_size = get_genome_size(params.genome_size, input.mash_report)
        avg_read_length = get_avg_read_length(params.fastp_report)
        down = get_down(params.downsample, genome_size, params.coverage_depth, avg_read_length)
        seed = get_seed(params.seed)
        print_downsample_values(genome_size, avg_read_length, params.coverage_depth, down, seed)
        shell("seqtk sample -s {seed} {input.reads} {down} > {params.outdir}/combined-reads.fq")
        shell("gzip -f {params.outdir}/combined-reads.fq")


rule spades:
    output:
        contigs="analysis/{sample}/single/{label}/spades/contigs.fasta"
    input:
        reads="seq/downsample/{sample}/single/combined-reads.fq.gz"
    threads: 128
    params:
        outdir="analysis/{sample}/single/{label}/spades",
        extra_args=lambda wildcards: config["assemblies"][wildcards.label].extra_args
    shell:
        """
        spades.py -s {input.reads} -t {threads} -o {params.outdir} {params.extra_args}
        """


rule megahit:
    output:
        contigs="analysis/{sample}/single/{label}/megahit/contigs.fasta"
    input:
        reads="seq/downsample/{sample}/single/combined-reads.fq.gz"
    threads: 128
    params:
        temp_dir="analysis/{sample}/single/{label}/megahit-temp",
        actual_dir="analysis/{sample}/single/{label}/megahit",
        extra_args=lambda wildcards: config["assemblies"][wildcards.label].extra_args
    shell:
        """
        megahit -r {input.reads} -t {threads} -o {params.temp_dir} {params.extra_args}
        mv {params.temp_dir}/* {params.actual_dir}
        rm -r {params.temp_dir}
        ln -s final.contigs.fa {output.contigs}
        """


rule unicycler:
    output:
        contigs="analysis/{sample}/single/{label}/unicycler/contigs.fasta"
    input:
        reads="seq/downsample/{sample}/single/combined-reads.fq.gz"
    threads: 128
    params:
        outdir="analysis/{sample}/single/{label}/unicycler",
        extra_args=lambda wildcards: config["assemblies"][wildcards.label].extra_args
    shell:
        """
        unicycler -s {input.reads} -t {threads} -o {params.outdir} {params.extra_args}
        ln -s assembly.fasta {output.contigs}
        """


rule velvet:
    output:
        contigs="analysis/{sample}/single/{label}/velvet/contigs.fasta"
    input:
        reads="seq/downsample/{sample}/single/combined-reads.fq.gz"
    threads: 128
    params:
        temp_dir="analysis/{sample}/single/{label}/velvet-temp",
        actual_dir="analysis/{sample}/single/{label}/velvet",
        extra_args=lambda wildcards: config["assemblies"][wildcards.label].extra_args,
        reads=lambda wildcards, input: f"'-short -fastq.gz {input.reads}'",
    shell:
        """
        VelvetOptimiser.pl -f {params.reads} -t {threads} -d {params.temp_dir} {params.extra_args}
        mv {params.temp_dir}/* {params.actual_dir}
        rm -r {params.temp_dir}
        ln -s contigs.fa {output.contigs}
        """

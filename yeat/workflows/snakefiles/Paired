# -------------------------------------------------------------------------------------------------
# Copyright (c) 2021, DHS. This file is part of YEAT: http://github.com/bioforensics/yeat
#
# This software was prepared for the Department of Homeland Security (DHS) by the Battelle National
# Biodefense Institute, LLC (BNBI) as part of contract HSHQDC-15-C-00064 to manage and operate the
# National Biodefense Analysis and Countermeasures Center (NBACC), a Federally Funded Research and
# Development Center.
# -------------------------------------------------------------------------------------------------

import json
import pandas as pd
from pathlib import Path
from random import randint
from shutil import copyfile


module helper:
    snakefile: "__init__.py"
    config: config

use rule * from helper


rule all:
    input:
        helper.get_expected_files(paired_qc=True)


module shared_workflow:
    snakefile: "Shared"
    config: config

use rule * from shared_workflow exclude copyinput as other_*


rule copyinput:
    output:
        read1="seq/input/{sample}/r1_combined-reads.fq.gz",
        read2="seq/input/{sample}/r2_combined-reads.fq.gz"
    input:
        r1s = lambda wildcards: [x[0] for x in config["reads"][wildcards.sample]["paired"]],
        r2s = lambda wildcards: [x[1] for x in config["reads"][wildcards.sample]["paired"]]
    params:
        outdir=lambda wildcards: f"seq/input/{wildcards.sample}"
    run:
        combine(input.r1s, "r1", params.outdir)
        combine(input.r2s, "r2", params.outdir)


def combine(reads, type1, outdir):
    for i, inread in enumerate(reads):
        outread = f"{outdir}/{type1}_reads{i}.fq"
        if inread.endswith(".gz"):
            shell("gunzip -c {inread} > {outread}")
        else:
            copyfile(inread, outread)
    shell(f"cat {outdir}/{type1}_reads*.fq > {outdir}/{type1}_combined-reads.fq")
    shell(f"gzip {outdir}/{type1}_combined-reads.fq")


rule fastqc:
    output:
        out1="seq/fastqc/{sample}/r1_combined-reads_fastqc.html",
        out2="seq/fastqc/{sample}/r2_combined-reads_fastqc.html"
    input:
        read1="seq/input/{sample}/r1_combined-reads.fq.gz",
        read2="seq/input/{sample}/r2_combined-reads.fq.gz"
    threads: 128
    params:
        outdir=lambda wildcards: f"seq/fastqc/{wildcards.sample}"
    shell:
        """
        fastqc {input.read1} {input.read2} -t {threads} -o {params.outdir}
        """


rule fastp:
    output:
        out1="seq/fastp/{sample}/{sample}.R1.fq.gz",
        out2="seq/fastp/{sample}/{sample}.R2.fq.gz"
    input:
        read1="seq/input/{sample}/r1_combined-reads.fq.gz",
        read2="seq/input/{sample}/r2_combined-reads.fq.gz"
    params:
        length_required=config["length_required"],
        html_report="seq/fastp/{sample}/fastp.html",
        json_report="seq/fastp/{sample}/fastp.json",
        txt_report="seq/fastp/{sample}/report.txt"
    shell:
        """
        fastp -i {input.read1} -I {input.read2} -o {output.out1} -O {output.out2} \
            -l {params.length_required} --detect_adapter_for_pe \
            --html {params.html_report} --json {params.json_report} \
            2> {params.txt_report}
        """


rule mash:
    output:
        sketch="seq/mash/{sample}/{sample}.R1.fq.gz.msh",
        mash_report="seq/mash/{sample}/{sample}.report.tsv"
    input:
        read1="seq/fastp/{sample}/{sample}.R1.fq.gz"
    shell:
        """
        mash sketch {input.read1} -o {output.sketch}
        mash info -t {output.sketch} > {output.mash_report}
        """


rule downsample:
    output:
        sub1="seq/downsample/{sample}/{sample}.R1.fq.gz",
        sub2="seq/downsample/{sample}/{sample}.R2.fq.gz"
    input:
        read1="seq/fastp/{sample}/{sample}.R1.fq.gz",
        read2="seq/fastp/{sample}/{sample}.R2.fq.gz",
        mash_report="seq/mash/{sample}/{sample}.report.tsv"
    params:
        coverage=config["coverage"],
        downsample=config["downsample"],
        fastp_report="seq/fastp/{sample}/fastp.json",
        genome_size=config["genomesize"],
        seed=config["seed"],
        outdir=lambda wildcards: f"seq/downsample/{wildcards.sample}"
    run:
        if params.downsample == -1:
            p = Path("seq/{wildcards.sample}/downsample")
            p.mkdir(parents=True, exist_ok=True)
            copyfile(input.read1, output.sub1)
            copyfile(input.read2, output.sub2)
            return
        if params.genome_size == 0:
            df = pd.read_csv(input.mash_report, sep="\t")
            genome_size = df["Length"].iloc[0]
        else:
            genome_size = params.genome_size
        with open(params.fastp_report, "r") as fh:
            qcdata = json.load(fh)
        base_count = qcdata["summary"]["after_filtering"]["total_bases"]
        read_count = qcdata["summary"]["after_filtering"]["total_reads"]
        avg_read_length = base_count / read_count
        if params.downsample == 0:
            down = int((genome_size * params.coverage) / (2 * avg_read_length))
        else:
            down = params.downsample
        if params.seed == "None":
            seed = randint(1, 2**16-1)
        else:
            seed = params.seed
        print(f"[yeat] genome size: {genome_size}")
        print(f"[yeat] average read length: {avg_read_length}")
        print(f"[yeat] target depth of coverage: {params.coverage}x")
        print(f"[yeat] number of reads to sample: {down}")
        print(f"[yeat] random seed for sampling: {seed}")
        shell("seqtk sample -s {seed} {input.read1} {down} > {params.outdir}/{wildcards.sample}.R1.fq")
        shell("seqtk sample -s {seed} {input.read2} {down} > {params.outdir}/{wildcards.sample}.R2.fq")
        shell("gzip -f {params.outdir}/*")


rule spades:
    output:
        contigs="analysis/{sample}/{label}/spades/{sample}_contigs.fasta"
    input:
        read1="seq/downsample/{sample}/{sample}.R1.fq.gz",
        read2="seq/downsample/{sample}/{sample}.R2.fq.gz"
    threads: 128
    params:
        outdir=lambda wildcards: f"analysis/{wildcards.sample}/{wildcards.label}/spades",
        extra_args=lambda wildcards: config["extra_args"][wildcards.label]
    shell:
        """
        spades.py -1 {input.read1} -2 {input.read2} -t {threads} -o {params.outdir} {params.extra_args}
        ln -s contigs.fasta {output.contigs}
        """


rule megahit:
    output:
        contigs="analysis/{sample}/{label}/megahit/{sample}_contigs.fasta"
    input:
        read1="seq/downsample/{sample}/{sample}.R1.fq.gz",
        read2="seq/downsample/{sample}/{sample}.R2.fq.gz"
    threads: 128
    params:
        temp_dir=lambda wildcards: f"analysis/{wildcards.sample}/{wildcards.label}/megahit-temp",
        actual_dir=lambda wildcards: f"analysis/{wildcards.sample}/{wildcards.label}/megahit",
        extra_args=lambda wildcards: config["extra_args"][wildcards.label]
    shell:
        """
        megahit -1 {input.read1} -2 {input.read2} -t {threads} -o {params.temp_dir} {params.extra_args}
        mv {params.temp_dir}/* {params.actual_dir}
        rm -r {params.temp_dir}
        ln -s final.contigs.fa {output.contigs}
        """


rule unicycler:
    output:
        contigs="analysis/{sample}/{label}/unicycler/{sample}_contigs.fasta"
    input:
        read1="seq/downsample/{sample}/{sample}.R1.fq.gz",
        read2="seq/downsample/{sample}/{sample}.R2.fq.gz"
    threads: 128
    params:
        outdir=lambda wildcards: f"analysis/{wildcards.sample}/{wildcards.label}/unicycler",
        extra_args=lambda wildcards: config["extra_args"][wildcards.label]
    shell:
        """
        unicycler -1 {input.read1} -2 {input.read2} -t {threads} -o {params.outdir} {params.extra_args}
        ln -s assembly.fasta {output.contigs}
        """
